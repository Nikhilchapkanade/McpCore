ğŸš€ MCP-RAG-Nexus

Welcome to MCP-RAG-Nexus! This is a fully dockerized proof-of-concept designed to explore the power of the Model Context Protocol (MCP).

I built this project to demonstrate how a local AI Agent (using Llama 3.2) can "wake up," realize it needs external information, and autonomously query a "Knowledge Server" to get answers. It simulates a real-world scenario where an AI looks up data from tools like Google Drive or Slack without hallucinating.

ğŸ§  How It Works

Think of this system as three friends talking in a private room (a Docker network):

The Brain (rag-agent): This is the LangChain agent. When you ask it a question, it decides if it needs to use a tool.

The Toolbox (mcp-knowledge): This acts as the "Knowledge Server." It holds mock data for "Google Drive" and "Slack" and serves it via MCP.

The Engine (ollama): This runs the actual Llama 3.2 model locally. It does the heavy lifting of processing text.

ğŸ› ï¸ Prerequisites

You don't need much to get this running. Just make sure you have:

Docker Desktop (This is essential as everything runs in containers).

Git (To clone the repo).

âš¡ Quick Start Guide

1. Clone the Repo
First, grab the code and jump into the directory:

Bash

git clone https://github.com/Nikhilchapkanade/mcp-rag-nexus.git
cd mcp-rag-nexus
2. Build the Stack
Fire up the containers. This might take a minute or two the first time as it downloads the Python libraries and sets up the network.

Bash

docker-compose up --build

3. âš ï¸ Important: Download the Brain
Don't skip this step! By default, the Ollama container starts empty. You need to pull the Llama 3.2 model inside the container.

While the containers are running, open a new terminal window and run:

Bash

docker exec -it mcp-rag-system-ollama-1 ollama pull llama3.2
(Note: If Docker complains about the name, run docker ps to double-check the Ollama container name).

ğŸ® Let's Test It!
Once everything is up and running (look for Uvicorn running on http://0.0.0.0:8080 in your logs), it's time to see the magic happen.

You can test it right from your browser or your terminal.

ğŸ§ª Scenario A: The "Google Drive" Search

Ask the agent about enterprise pricing. It will realize it doesn't know the answer, call the "Product Docs" tool, and give you the mock data.

Browser Link: Click here to ask about Pricing

Terminal Command:

Bash

curl "http://localhost:8080/query?q=What%20is%20the%20pricing%20for%20Enterprise"
Expected Response:

"The Enterprise plan costs $50/user/month."

ğŸ§ª Scenario B: The "Slack" Check

Ask about the dev team's progress. The agent will switch tools and search the "Slack" history.

Browser Link: Click here to ask about the Memory Leak

ğŸ“‚ Project Structure

Here is a quick look at how the code is organized if you want to tinker with it:

Plaintext

â”œâ”€â”€ docker-compose.yml      # The blueprint that connects the 3 services
â”œâ”€â”€ rag-agent/              # The "Brain"
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ server.py           # FastAPI server handling your requests
â”‚   â”œâ”€â”€ agent.py            # Where LangChain decides which tool to use
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ mcp-knowledge/          # The "Toolbox"
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ main.py             # Defines the mock Google/Slack tools via FastMCP
    â””â”€â”€ requirements.txt
ğŸ‘‹ Contributing
This is a playground project! Feel free to fork it, add new tools to the mcp-knowledge folder, or swap out Llama 3.2 for a different model. If you run into issues, open an issue or drop a PR!

Happy coding! ğŸš€
